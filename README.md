# gograd

gograd is a lightweight and simplified implementation of a deep learning library in Go. It is inspired by the original TinyGrad library and provides a minimalistic yet functional framework for building and training neural networks.

## Features

- TODO: Backpropagation: Automatic differentiation for computing gradients of the loss function with respect to the model parameters.
- TODO: Optimization: Stochastic Gradient Descent (SGD) optimization for updating the model parameters during training.
- TODO: Dense Layer: Implementation of a fully connected layer that can be used to build feedforward neural networks.
- TODO: Activation Functions: Support for common activation functions like ReLU, Sigmoid, and Tanh.
- TODO: Loss Functions: Built-in loss functions such as Mean Squared Error (MSE) and Cross-Entropy.
- TODO: Add Batch Normalization for improved training performance and stability.

## Getting Started

### Installation

To use gograd, you need to have Go installed on your system. If you don't have it, you can download it from the official website: https://golang.org/

Once you have Go installed, you can install gograd using `go get`:
```bash
go get github.com/campv/gograd
```

## Contributing
Contributions to gograd are welcome! If you have suggestions, bug reports, or improvements, please open an issue or submit a pull request.

## License
gograd is licensed under the MIT License.

## Acknowledgments
This project is inspired by the original micrograd library [micrograd library](https://github.com/karpathy/micrograd) and [tinygrad library](https://github.com/tinygrad/tinygrad)
